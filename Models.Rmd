---
title: "Models"
author: "Owen Fiore"
date: "2023-12-19"
output: html_document
---

```{r}
df <- read.csv("DataToModel.csv")
```


```{r}
predictor_variables <- df[, c("MathScore", "ReadingScore", "WritingScore", "AggregateScore")]
cor(predictor_variables)
```



```{r}
aovpe <- aov(AggregateScore ~ ParentEduc, data = df)
summary(lm(aovpe))
```
```{r}
library(car)
aovres <- residuals(aovpe)
car::qqPlot(aovres, main = NA, pch = 19, col = 2, cex = 0.7)
```
This is a significant departure from normality
We cannot use the Shapio-Wilk signficance test as the sample size is too large, but it is clear that the residuals are not normal.

```{r}
#leveneTest(AggregateScore ~ ParentEduc, data = df)
```
We test $H_0: \sigma_1^2 = \sigma_2^2 = ... = \sigma_k^2$ for $k$ groups
As the p value is larger than 0.05, this supports the assumption that the variances are equal.

As assumption of normality is violated, instead we try the kruskal-wallis nonparametric test
```{r}
kruskal.test(AggregateScore ~ ParentEduc, data = df)
```
## Linear Models
First we do train test split
```{r}
set.seed(123457)
train.prop <- 0.80
strats <- df$AggregateScore
rr <- split(1:length(strats), strats)
idx <- sort(as.numeric(unlist(sapply(rr, 
        function(x) sample(x, length(x)*train.prop)))))
df.train <- df[idx, ]
df.test <- df[-idx, ]
```



```{r}
predictors <- c("Male", "ParentEduc", "LunchDiscount", "TestPrep", "PracticeSport", "IsFirstChild", "NrSiblings", "PrivateTransport", "WklyStudyHours", "EthnicGroupA", "EthnicGroupB", "EthnicGroupC", "EthnicGroupD", "EthnicGroupE", "ParentDivorced", "ParentMarried", "ParentSingle", "ParentWidowed" )

all <- glm(AggregateScore ~ Male + ParentEduc + LunchDiscount + TestPrep + PracticeSport + IsFirstChild + NrSiblings + PrivateTransport + WklyStudyHours + EthnicGroupA + EthnicGroupB + EthnicGroupC + EthnicGroupD + EthnicGroupE + ParentDivorced + ParentMarried + ParentSingle + ParentWidowed, data = df.train, family = gaussian(link = "identity"))
summary(all)
```
```{r}
par(mfrow = c(2,2))
plot(all)
```

```{r}
step_model <- step(all, direction = "both", trace = 0)
summary(step_model)
```
```{r}
par(mfrow = c(2,2))
plot(step_model)
```
```{r}
test_predictions <- predict(step_model, newdata = df.test, type = "link")
test_results <- data.frame(Actual = df.test$AggregateScore, Predicted = test_predictions)
test_results
```
```{r}
plot(test_results$Actual, test_results$Predicted, col="grey33", cex=0.3, xlab="Actual", ylab="Predicted")
abline(0,1)
```
```{r}
car::vif(step_model)
```

There does not appear to be multicollinearity between the predictors as we would expect VIF to be larger than 1.

```{r}
mse <- mean((test_results$Actual - test_results$Predicted)^2)
mse
```

## Random Forests
```{r}
set.seed(1)
library(ranger)
rf_step <- ranger(AggregateScore ~ Male + ParentEduc + LunchDiscount + TestPrep + PracticeSport + IsFirstChild + WklyStudyHours + EthnicGroupA + EthnicGroupB + EthnicGroupC + EthnicGroupD + EthnicGroupE, data = df.train, importance = "impurity")
rf_step
```


Grid Search Hyperparameter tuning
```{r}
set.seed(1)
library(ranger)

# Define your custom function for training a random forest
train_rf <- function(num.trees, min.node.size) {
  
  
  # Create the random forest model
  rf_model <- ranger(AggregateScore ~ Male + ParentEduc + LunchDiscount + TestPrep + PracticeSport + IsFirstChild + WklyStudyHours + EthnicGroupA + EthnicGroupB + EthnicGroupC + EthnicGroupD + EthnicGroupE,
                     data = df.train,
                     importance = "impurity",
                     num.trees = num.trees,
                     min.node.size = min.node.size)
  
  # Calculate OOB error (MSE)
  oob_error <- sqrt(rf_model$prediction.error)
  
  # Return the trained model and OOB error
  return(list(model = rf_model, oob_error = oob_error))
}

# Example usage:
# Replace 'your_data' with your actual dataset and 'your_target_col' with your target variable
# Set the hyperparameter values you want to try
num_trees_values <- c(300, 500, 700, 900)
min_node_size_values <- c(20, 25, 30, 35)

# Initialize variables to keep track of the best model and its OOB error
best_model <- NULL
best_oob_error <- Inf

# Perform grid search
for (num_trees in num_trees_values) {
  for (min_node_size in min_node_size_values) {
    result <- train_rf(num_trees, min_node_size)
    model <- result$model
    oob_error <- result$oob_error
    
    # Check if the current model has a lower OOB error
    if (oob_error < best_oob_error) {
      best_model <- model
      best_oob_error <- oob_error
    }
  }
}

# The best model is stored in 'best_model' with the lowest OOB error
print(best_model)
```

```{r}
library(vip)
importance <- vi(best_model)
importance
```
```{r}
vip(importance)
```

